Alloy Composition Design and Optimization using Variational Information Bottleneck and Attention-Based Neural Networks
Overview
This project aims to design and optimize new alloy compositions, focusing on achieving high hardness values for multicomponent metallic glasses or alloys. By leveraging Variational Information Bottleneck (VIB) and attention-based neural networks, the model predicts hardness from composition and load data, while generating new, optimized compositions through exploration of the latent space.

Model Architecture
Variational Information Bottleneck (VIB) Layer
The VIB layer captures only the most relevant information from the input, balancing reconstruction loss with a KL-divergence regularization term controlled by a dynamic beta parameter.

KL Annealing and Dynamic Beta Adjustment: These techniques gradually increase the influence of KL divergence during training, enhancing the model's robustness by encouraging a compact and relevant latent representation.
Attention Mechanism

Feature-Wise Attention: A custom attention layer is applied separately to composition and load features, highlighting influential features and improving interpretability by providing attention scores for each feature.
Multi-Head Attention: After the VIB layer, multi-head attention captures dependencies within the latent space, refining the information in the latent vectors.
Output Layer
Predicts hardness (HV) based on latent representations, with mean squared error (MSE) used as the primary loss function.

Training Process
Loss Tracking:
Training and validation losses are tracked to monitor convergence. Beta values and KL loss are recorded across epochs to observe the effect of KL divergence on the latent representations.

Attention Score Logging:
During each epoch, attention scores for composition features are logged, enabling post-training analysis of feature importance.

Early Stopping and Learning Rate Scheduling:
These techniques prevent overfitting and ensure efficient convergence.

Model Interpretability
Integrated Gradients (IG):
Used to measure feature attributions, IG provides insights into how much each input feature contributes to the model's output.

Attention Visualization:
Visualizations (heatmaps and bar plots) show the importance of each feature across samples, based on attention scores, improving the model's interpretability.

Inverse Design for New Alloy Compositions
Latent Space Sampling
Cluster-Based Sampling:
The VIB layer's latent space was explored by sampling around cluster centers derived from high-hardness samples. New latent vectors were generated by sampling within these clusters, aiming to generate compositions likely to achieve high hardness values.
Optimization of Latent Vectors
Gradient-Based Optimization:
Starting with initial latent vectors around high-hardness points, a gradient-based optimization loop minimizes the mean squared error between predicted hardness and a target hardness (e.g., 2000 HV).

Latent Vector Constraints:
Constraints are applied to ensure decoded compositions sum to 1, maintaining physical validity.

Decoding to Compositions
Decoding and Validation:
The optimized latent vectors are decoded back to the composition space using the decoder model. The decoded compositions are validated to ensure physical plausibility (e.g., fractions summing to 1).
